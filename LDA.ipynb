{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd017767560fef6456e08416b833cd60cbe97e29174fd16fff4cbad564a6fa9df30",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "17767560fef6456e08416b833cd60cbe97e29174fd16fff4cbad564a6fa9df30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import re\r\n",
    "\r\n",
    "posts = pd.read_csv('Cleaned Posts/PostsOnly-wallstreetbets-NOAutoMod.csv')\r\n",
    "comments = pd.read_csv('Cleaned Comments/CommentsOnly-wallstreetbets-NOAutoMod.csv')\r\n",
    "\r\n",
    "postsData = posts.copy()\r\n",
    "postsData['title'] = posts['title'].fillna('').astype(str) + \" \" + posts['selftext'].fillna('').astype(str)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# bodyComments = comments['body']\r\n",
    "# postsData = postsData['title']\r\n",
    "\r\n",
    "# print(bodyComments.shape)\r\n",
    "# print(postsData.shape)\r\n",
    "\r\n",
    "# subData = postsData.append(bodyComments)\r\n",
    "\r\n",
    "# print(subData.shape)\r\n",
    "\r\n",
    "\r\n",
    "documentArray = []\r\n",
    "\r\n",
    "\r\n",
    "linkIDs = comments['link_id'].unique().tolist()\r\n",
    "\r\n",
    "counter = 0\r\n",
    "\r\n",
    "for item in linkIDs:\r\n",
    "    returnString = \"\"\r\n",
    "    \r\n",
    "    \r\n",
    "    ###Get the comments of the specific post link_id\r\n",
    "    commentsofPost = comments[comments['link_id'] == item].fillna('').astype(str)\r\n",
    "    for index, row in commentsofPost.iterrows():\r\n",
    "        #Remove URLs from strings\r\n",
    "        singleComment = str(row['body'])\r\n",
    "        removedURL = re.sub(r\"http\\S+\", \"\", singleComment)\r\n",
    "        returnString += removedURL + \" \"\r\n",
    "    ################################################\r\n",
    "    \r\n",
    "    \r\n",
    "    ###Get Post and post body of post link_id \r\n",
    "    postTitleBody = postsData[postsData['id'] == item].fillna('').astype(str)\r\n",
    "    postString = str(postTitleBody.iloc[0]['title']) + \" \" + str(postTitleBody.iloc[0]['selftext'])\r\n",
    "    postString = re.sub(r\"http\\S+\", \"\", postString)\r\n",
    "    returnString = postString + returnString\r\n",
    "    ################################################\r\n",
    "\r\n",
    "    documentArray.append(str(returnString))\r\n",
    "\r\n",
    "#Convert to pandas\r\n",
    "subDocuments = pd.DataFrame(documentArray)\r\n",
    "subDocuments = subDocuments.squeeze()\r\n",
    "print(subDocuments.shape)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Data Pre-processing\r\n",
    "\r\n",
    "import gensim\r\n",
    "from gensim.utils import simple_preprocess\r\n",
    "from gensim.parsing.preprocessing import STOPWORDS\r\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\r\n",
    "from nltk.stem.porter import *\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "import nltk\r\n",
    "nltk.download('wordnet')\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "stemmer = PorterStemmer()\r\n",
    "\r\n",
    "\r\n",
    "my_stop_words = STOPWORDS.union(set(['like']))\r\n",
    "\r\n",
    "def lemmatize_stemming(text):\r\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\r\n",
    "def preprocess(text):\r\n",
    "    result = []\r\n",
    "    for token in gensim.utils.simple_preprocess(text):\r\n",
    "        # if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\r\n",
    "        if token not in my_stop_words and len(token) > 2:\r\n",
    "            result.append(lemmatize_stemming(token))\r\n",
    "    return result\r\n",
    "\r\n",
    "\r\n",
    "# doc_sample = selfTextPosts.iloc[5]\r\n",
    "\r\n",
    "# print('original document: ')\r\n",
    "# words = []\r\n",
    "# for word in doc_sample.split(' '):\r\n",
    "#     words.append(word)\r\n",
    "# print(words)\r\n",
    "# print('\\n\\n tokenized and lemmatized document: ')\r\n",
    "# print(preprocess(doc_sample))\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "processed_docs = subDocuments.map(preprocess)\r\n",
    "print(processed_docs[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\r\n",
    "count = 0\r\n",
    "for k, v in dictionary.iteritems():\r\n",
    "    print(k, v)\r\n",
    "    count += 1\r\n",
    "    if count > 10:\r\n",
    "        break\r\n",
    "\r\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\r\n",
    "\r\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\r\n",
    "print(bow_corpus[41])\r\n",
    "\r\n",
    "\r\n",
    "bow_doc_80 = bow_corpus[41]\r\n",
    "for i in range(len(bow_doc_80)):\r\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_80[i][0], \r\n",
    "                                               dictionary[bow_doc_80[i][0]], bow_doc_80[i][1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim import corpora, models\r\n",
    "tfidf = models.TfidfModel(bow_corpus)\r\n",
    "corpus_tfidf = tfidf[bow_corpus]\r\n",
    "from pprint import pprint\r\n",
    "for doc in corpus_tfidf:\r\n",
    "    pprint(doc)\r\n",
    "    break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=8, id2word=dictionary, passes=20, iterations=400, workers=4)\r\n",
    "\r\n",
    "# print(lda_model.print_topics(num_topics=5, num_words=5))\r\n",
    "\r\n",
    "for idx, topic in lda_model.print_topics(num_topics=7, num_words=7):\r\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=8, id2word=dictionary,passes=20, iterations=400, workers=4)\r\n",
    "\r\n",
    "for idx, topic in lda_model_tfidf.print_topics(num_topics=7, num_words=7):\r\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}